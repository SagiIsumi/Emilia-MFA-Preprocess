import gc
import json
import logging
import os
import re
import shutil
import subprocess
import tarfile
from pathlib import Path
from typing import Set

import opencc
import textgrid
from pypinyin import Style, load_phrases_dict, pinyin
from transformers import AutoTokenizer

# å‡è¨­é€™äº›å¾ä½ çš„æ¨¡çµ„åŒ¯å…¥
from Audio_pretrain.timestamping.emilia_unpackage import (
    iter_emilia_tar,
    save_debug_sample,
    transcode_mp3_to_wav_bytes,
    write_webdataset_sample,
)

# --- è¨­å®šå€ ---
TAR_PATH = "/mnt/data_hdd/.cache/huggingface/datasets/Emilia/ZH/ZH-B000000.tar"
TEMP_DIR_ZH = "./mfa_outputs/temp/zh"
TEMP_DIR_EN = "./mfa_outputs/temp/en"
OUTPUT_DIR = "./mfa_outputs/results"
OUTPUT_TAR_ROOT = "/mnt/data_ssd/Emilia/ZH"  # æœ€çµ‚è¼¸å‡º tar çš„æ ¹ç›®éŒ„
CHECKPOINT_FILE = "./mfa_outputs/processed_ids.txt" # ç´€éŒ„å·²æˆåŠŸå°é½Šçš„ ID
LOG_FILE = "./mfa_outputs/processing.log"
TARGET_SR = 16000
MAX_SAMPLES = 2000  # å¢åŠ æ¨£æœ¬æ•¸ä»¥å±•ç¤º Resume å¨åŠ›
CUSTOM_PHRASES = {}
converter = opencc.OpenCC('s2twp.json')

# --- Logger è¨­å®š ---
os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("Emilia_MFA")

# --- Tokenizer åˆå§‹åŒ–èˆ‡åŸåœ°æ›¿æ› ---
def get_custom_tokenizer():
    tok = AutoTokenizer.from_pretrained("MediaTek-Research/Llama-Breeze2-3B-Instruct")
    return tok

tokenizer = get_custom_tokenizer()

def load_checkpoints(file_path: str) -> Set[str]:
    if os.path.exists(file_path):
        with open(file_path, "r") as f:
            return set(line.strip() for line in f if line.strip())
    return set()

def save_checkpoint(file_path: str, sample_id: str):
    with open(file_path, "a") as f:
        f.write(f"{sample_id}\n")

# ... [ä¿ç•™ä½ çš„ clean_text å’Œ convert_to_mfa_pinyin å‡½æ•¸] ...

def clean_text(text):
    text = re.sub(r"[^\w\s\u4e00-\u9fa5]", " ", text)
    return " ".join(text.split())

def convert_to_mfa_pinyin(text):
    if CUSTOM_PHRASES: load_phrases_dict(CUSTOM_PHRASES)
    clean = clean_text(text)
    raw_pinyins = pinyin(clean, style=Style.TONE3)
    final_pinyins = []
    for p in raw_pinyins:
        if p[0] != ' ' and not p[0][-1].isdigit():
            final_pinyins.append(p[0] + '5')
        else:
            final_pinyins.append(p[0])
    return " ".join(final_pinyins)

def run_mfa(temp_dir: str, out_dir: str, model_name: str):
    logger.info(f"Step 2: åŸ·è¡Œ MFA å°é½Š. è¼¸å…¥: {temp_dir}")
    cmd = [
        "mfa", "align", str(temp_dir), model_name, model_name, str(out_dir),
        "--clean", "--beam", "40", "--retry-beam", "150", "--j", "4"
    ]
    try:
        subprocess.run(cmd, check=True, capture_output=True, text=True)
        logger.info("MFA å°é½Šæ‰¹æ¬¡åŸ·è¡ŒæˆåŠŸ")
    except subprocess.CalledProcessError as e:
        logger.error(f"MFA å¤±æ•—: {e.stderr}")
        raise e

def textgrid_to_moshi_format(tg_path, frame_rate=12.5):
    # ä½¿ç”¨ reserved_4 ä½œç‚ºé è¨­ pad (å°æ‡‰ä¸Šé¢æ›¿æ›å¾Œçš„ <|pad|>)
    PAD_TOKEN = "<|reserved_special_token_4|>" 
    EPAD_TOKEN = "<|reserved_special_token_5|>"
    
    tg = textgrid.TextGrid.fromFile(tg_path)
    words_tier = tg[0]
    total_duration = tg.maxTime
    total_frames = int(total_duration * frame_rate)
    
    moshi_sequence = [PAD_TOKEN] * total_frames
    
    for interval in words_tier:
        start_frame = int(interval.minTime * frame_rate)
        end_frame = min(int(interval.maxTime * frame_rate), total_frames)
        text = interval.mark.strip()
        
        if text:
            # ç¬¬ä¸€å¹€æ”¾æ–‡å­—ï¼Œä¹‹å¾Œæ”¾ <pad> (é€™è£¡åŸæœ¬åˆå§‹åŒ–å°±æ˜¯ padï¼Œæ‰€ä»¥åªéœ€æ”¾æ–‡å­—)
            moshi_sequence[min(start_frame, total_frames-1)] = converter.convert(text)
            # å¦‚æœæ˜¯åœé “å‰ï¼Œæ¨™è¨˜ç‚º epad
            if start_frame > 0 and moshi_sequence[start_frame - 1] == PAD_TOKEN:
                moshi_sequence[start_frame - 1] = EPAD_TOKEN
                
    return moshi_sequence

def language_ratio(text):
    # ç§»é™¤ç©ºç™½
    text = text.replace(" ", "")
    if not text: return "Empty"
    
    chinese_chars = len(re.findall(r'[\u4e00-\u9fa5]', text))
    english_chars = len(re.findall(r'[a-zA-Z]', text))
    
    total = len(text)
    cn_rate = chinese_chars / total
    en_rate = english_chars / total
    
    if cn_rate > 0.5:
        return "zh"
    elif en_rate > 0.8: # è‹±æ–‡é€šå¸¸åŒ…å«ç©ºæ ¼èˆ‡æ¨™é»ï¼Œæ¯”ä¾‹è¨­é«˜ä¸€é»
        return "en"
    return "hybrid"

def purge_mfa_working_dirs():
    """å°ˆé–€æ¸…ç†å°è‡´é•·è·‘å´©æ½°çš„è³‡æ–™åº«èˆ‡æš«å­˜ï¼Œä¿ç•™æ¨¡å‹æª”æ¡ˆ"""
    mfa_root = Path("~/Documents/MFA").expanduser()
    
    # æ ¹æ“šä½ çš„ ls çµæœï¼Œé€™äº›æ˜¯æœƒç”¢ç”Ÿ zh.db èˆ‡æš«å­˜çš„åœ°æ–¹
    # ç•¶ä½ è™•ç†è‹±æ–‡æ™‚ï¼Œè«‹æŠŠ "en" ä¹ŸåŠ å…¥
    dirs_to_purge = ["zh", "en", "temp", "joblib_cache"]
    
    for d in dirs_to_purge:
        target = mfa_root / d
        if target.exists():
            try:
                shutil.rmtree(target)
                print(f"ğŸ§¹ å·²æ¸…é™¤å°é½Šå¿«å–: {target.name}")
            except Exception as e:
                # æœ‰æ™‚å› ç‚ºæ¬Šé™æˆ–é–å®šæœƒå¤±æ•—ï¼Œæˆ‘å€‘è¨˜éŒ„ä¸‹ä¾†
                print(f"âš ï¸ ç„¡æ³•æ¸…ç† {target.name}: {e}")

    # å¦å¤–ç‰¹åˆ¥æª¢æŸ¥ command_history é€™ç¨®æœƒç·©æ…¢å¢é•·çš„æª”æ¡ˆ
    history_file = mfa_root / "command_history.yaml"
    if history_file.exists():
        history_file.unlink()

if __name__ == "__main__":
    processed_ids = load_checkpoints(CHECKPOINT_FILE)
    logger.info(f"å·²å¾ Checkpoint è¼‰å…¥ {len(processed_ids)} ç­†è³‡æ–™ï¼Œå°‡è·³éå·²è™•ç†é …ã€‚")
    
    
    all_files = Path("/mnt/data_hdd/.cache/huggingface/datasets/Emilia/ZH").glob("*.tar")
    for tar_path in all_files:
        if str(tar_path) in processed_ids:
            logger.info(f"è·³éå·²è™•ç†æª”æ¡ˆ: {tar_path.name}")
            continue
        
        # æ¸…é™¤é•·æœŸå¿«å–
        purge_mfa_working_dirs()
        
        logger.info(f"é–‹å§‹è™•ç†æª”æ¡ˆ: {tar_path.name}")
        pair_generator = iter_emilia_tar(tar_path=tar_path)
        
        # å»ºç«‹è‡¨æ™‚è³‡æ–™å¤¾ï¼ˆåƒ…å­˜æ”¾æœ¬æ¬¡éœ€è¦è™•ç†çš„ï¼‰
        if os.path.exists(TEMP_DIR_ZH): shutil.rmtree(TEMP_DIR_ZH)
        os.makedirs(TEMP_DIR_ZH, exist_ok=True)
        if os.path.exists(TEMP_DIR_EN): shutil.rmtree(TEMP_DIR_EN)
        os.makedirs(TEMP_DIR_EN , exist_ok=True)
        if os.path.exists(OUTPUT_DIR): shutil.rmtree(OUTPUT_DIR)
        os.makedirs(OUTPUT_DIR, exist_ok=True)

        current_batch_ids = {
            "zh": [],
            "en": [],
        }
        
        
        file_counter = 0
        # --- Step 1: æ”¶é›†å°šæœªè™•ç†çš„æ•¸æ“š ---
        for idx, (sample_id, mp3_bytes, json_bytes) in enumerate(pair_generator):
            # Debugging : use fewer samples
            # if (len(current_batch_ids["zh"]) + len(current_batch_ids["en"])) >= MAX_SAMPLES:
            #     break

            # logger.info(f"æ­£åœ¨é è™•ç†: {sample_id}")
            wav_bytes, duration = transcode_mp3_to_wav_bytes(mp3_bytes, TARGET_SR)
            metadata = json.loads(json_bytes.decode("utf-8"))

            metadata["text"] = clean_text(metadata["text"])
            lang = language_ratio(metadata["text"])
            metadata["language"] = lang
            metadata["duration"] = duration
            speaker = metadata["speaker"]

            
            if lang == "zh":
                current_batch_ids["zh"].append(sample_id)
                save_debug_sample(os.path.join(TEMP_DIR_ZH, speaker), sample_id, wav_bytes, metadata, mode="mfa")
            elif lang == "en":
                current_batch_ids["en"].append(sample_id)
                save_debug_sample(os.path.join(TEMP_DIR_EN, speaker), sample_id, wav_bytes, metadata, mode="mfa")
            file_counter += 1
            # --- Step 2: åŸ·è¡Œ MFA ---
            if file_counter>= MAX_SAMPLES:
                if Path(TEMP_DIR_ZH).exists() and any(Path(TEMP_DIR_ZH).iterdir()):
                    run_mfa(TEMP_DIR_ZH, OUTPUT_DIR, model_name="mandarin_mfa")
                if Path(TEMP_DIR_EN).exists() and any(Path(TEMP_DIR_EN).iterdir()):
                    run_mfa(TEMP_DIR_EN, OUTPUT_DIR, model_name="english_us_arpa")
                
                # æ›´æ–°å¿«å–è³‡æ–™å¤¾ï¼ˆåƒ…å­˜æ”¾æœ¬æ¬¡éœ€è¦è™•ç†çš„ï¼‰
                if os.path.exists(TEMP_DIR_ZH): shutil.rmtree(TEMP_DIR_ZH)
                os.makedirs(TEMP_DIR_ZH, exist_ok=True)
                if os.path.exists(TEMP_DIR_EN): shutil.rmtree(TEMP_DIR_EN)
                os.makedirs(TEMP_DIR_EN , exist_ok=True)
                
                # é‡æ–°åˆå§‹åŒ–counter
                file_counter = 0

        # --- Step 2: åŸ·è¡Œ MFA ---
        if Path(TEMP_DIR_ZH).exists() and any(Path(TEMP_DIR_ZH).iterdir()):
            run_mfa(TEMP_DIR_ZH, OUTPUT_DIR, model_name="mandarin_mfa")
        if Path(TEMP_DIR_EN).exists() and any(Path(TEMP_DIR_EN).iterdir()):
            run_mfa(TEMP_DIR_EN, OUTPUT_DIR, model_name="english_us_arpa")
        
        moshi_tokens ={}
        # --- Step 3: è½‰æ›èˆ‡ Tokenize ---
        merge_ids = current_batch_ids["zh"] + current_batch_ids["en"]
        for sample_id in merge_ids:
            speaker = sample_id[:len(sample_id)-len(sample_id.split("_")[-1])-1]  # å‡è¨­ ID æ ¼å¼ç‚º speaker_XXX
            textgrid_root = os.path.join(OUTPUT_DIR, speaker)
            textgrid_path = os.path.join(textgrid_root, f"{sample_id}.TextGrid")
            
            if not os.path.exists(textgrid_path):
                logger.warning(f"è·³é {sample_id}: MFA æœªç”Ÿæˆ TextGrid (å¯èƒ½å°é½Šå¤±æ•—)")
                continue

            # logger.info(f"æ­£åœ¨è½‰æ›æ ¼å¼ä¸¦å„²å­˜çµæœ: {sample_id}")
            moshi_sequence = textgrid_to_moshi_format(textgrid_path)
            
            token_list = []
            # ä½¿ç”¨ä½ çš„é•·åº¦è£œå„Ÿé‚è¼¯
            pad_remove_counter = 0
            for token in moshi_sequence:
                if pad_remove_counter > 0:
                    pad_remove_counter -= 1
                    continue
                # add_special_tokens=False ç¢ºä¿ä¸æœƒå™´å‡º BOS [128000]
                ids = tokenizer.encode(token, add_special_tokens=False)
                pad_remove_counter = len(ids) - 1
                token_list.extend(ids)

            # é©—è­‰èˆ‡å­˜æª”é‚è¼¯
            try:
                assert len(token_list) == len(moshi_sequence), f"é•·åº¦ä¸åŒ¹é… {len(token_list)} vs {len(moshi_sequence)}"
                # é€™è£¡å¯ä»¥åŠ å…¥ä½ å¯«å…¥ WebDataset æˆ– Final JSON çš„é‚è¼¯
            except AssertionError as e:
                logger.error(f"{sample_id} è™•ç†å¤±æ•—: {str(e)}")
                token_list = []  # æˆ–è€…å…¶ä»–çš„ fallback ç­–ç•¥
            moshi_tokens[sample_id] = token_list
                
        logger.info(f"å®Œæˆè™•ç†æª”æ¡ˆ: {tar_path.name}ï¼ŒæˆåŠŸå°é½Š {len(merge_ids)} ç­†è³‡æ–™ã€‚")
        
        
        # --- Step 4: Save as tar file ---
        pair_generator = iter_emilia_tar(tar_path=tar_path)
        output_tar_path = os.path.join(OUTPUT_TAR_ROOT, f"{tar_path.stem}.tar")
        with tarfile.open(output_tar_path, "w") as tar_handle:
            for idx, (sample_id, mp3_bytes, json_bytes) in enumerate(pair_generator):
                if sample_id not in merge_ids:
                    # logger.info(f"è·³é {sample_id}ï¼Œå› ç‚ºå®ƒä¸åœ¨æœ¬æ‰¹æ¬¡çš„å°é½Šçµæœä¸­ã€‚")
                    continue
                
                # 1. è½‰ç¢¼ MP3 ç‚º WAV bytes
                wav_bytes, duration = transcode_mp3_to_wav_bytes(mp3_bytes, TARGET_SR)

                # 2. è§£æ JSON Metadata
                metadata = json.loads(json_bytes.decode("utf-8"))
                metadata["duration"] = duration  # å¯ä»¥é¸æ“‡å°‡éŸ³è¨Šé•·åº¦åŠ å…¥ Metadata
                if sample_id in current_batch_ids["zh"]:
                    metadata["language"] = "zh"
                elif sample_id in current_batch_ids["en"]:
                    metadata["language"] = "en"
                else:
                    continue  # ç†è«–ä¸Šä¸æ‡‰è©²ç™¼ç”Ÿï¼Œå› ç‚ºå‰é¢å·²ç¶“éæ¿¾éäº†
                metadata["padded_tokens"] = moshi_tokens.get(sample_id, [])
                if not metadata["padded_tokens"]:
                    logger.warning(f"{sample_id} æ²’æœ‰æˆåŠŸç”Ÿæˆ padded_tokensï¼Œå°‡åœ¨å¾ŒçºŒæª¢æŸ¥ä¸­è¢«æ¨™è¨˜ã€‚")
                    continue
                # else:
                #     logger.info(f"{sample_id} æˆåŠŸç”Ÿæˆ padded_tokensï¼Œå°‡è¢«åŒ…å«åœ¨è¼¸å‡ºä¸­ã€‚")
                
                # 3. [Streaming] å¯«å…¥ WebDataset Tar (å¦‚æœéœ€è¦)
                write_webdataset_sample(tar_handle, sample_id, wav_bytes, metadata)
        save_checkpoint(CHECKPOINT_FILE, str(tar_path))
        gc.collect()  # å˜—è©¦é‡‹æ”¾è¨˜æ†¶é«”ï¼Œå°¤å…¶æ˜¯åœ¨è™•ç†å¤§å‹æª”æ¡ˆå¾Œ
    logger.info("æœ¬æ‰¹æ¬¡ä»»å‹™åŸ·è¡Œå®Œç•¢ã€‚")